= {productname} in the Public Cloud: {productname} Server and {productname} Proxy in the Public Cloud
ifdef::env-github,backend-html5,backend-docbook5[]
//Admonitions
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
:linkattrs:
// SUSE ENTITIES FOR GITHUB
// System Architecture
:zseries: z Systems
:ppc: POWER
:ppc64le: ppc64le
:ipf : Itanium
:x86: x86
:x86_64: x86_64
// Rhel Entities
:rhel: Red Hat Enterprise Linux
:rhnminrelease6: Red Hat Enterprise Linux Server 6
:rhnminrelease7: Red Hat Enterprise Linux Server 7
// SUSE Manager Entities
:productname:
:susemgr: SUSE Manager
:susemgrproxy: SUSE Manager Proxy
:productnumber: 3.2
:saltversion: 2018.3.0
:webui: WebUI
// SUSE Product Entities
:sles-version: 12
:sp-version: SP3
:jeos: JeOS
:scc: SUSE Customer Center
:sls: SUSE Linux Enterprise Server
:sle: SUSE Linux Enterprise
:slsa: SLES
:suse: SUSE
:ay: AutoYaST
endif::[]
// Asciidoctor Front Matter
:doctype: book
:sectlinks:
:toc: left
:icons: font
:experimental:
:sourcedir: .
:imagesdir: images



[abstract]
--
{productname} delivers best-in-class Linux server management capabilities.
For detailed information about the product please refer to the https://www.suse.com/documentation/suse_manager[SUSEManager] documentation.

The {productname} Server and {productname} Proxy images published by SUSE in selected Public Cloud environments are provided as Bring Your Own Subscription (BYOS) images.
{productname} Server instances need to be registered with the SUSE Customer Center (SCC).
Subscriptions of {productname} Proxy instances are handled through their parent {productname} Server.
After an instance is launched, {productname} needs to be set up and configured following the procedure in the {productname} documentation.
--
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:imagesdir: ./images

[[instance-requirements]]
== Instance Requirements


Select an instance size that meets the system requirements as documented in the {productname} documentation.

* Minimal main memory: >12G
* The {productname} setup procedure performs a Forward-confirmed reverse DNS lookup. This must succeed in order for the setup procedure to complete successfully and for {productname} to operate as expected. Therefore it is important that the hostname and IP configuration be performed prior to running the {productname} setup procedure.
* {productname} Server and {productname} Proxy instances are expected to run in a network configuration that provides you control over DNS entries and that is shielded from the Internet at large. Within this network configuration DNS (Domain Name Service) resolution must be provided, such that `hostname -f` returns the FQDN (Full Qualified Domain Name). The DNS resolution is not only important for the {productname} Server procedure but is also important when clients are configured to be managed via {productname}. Configuring DNS is Cloud Framework dependent, please refer to the cloud service provider documentation for detailed instructions.
* Minimal free disk space for {productname} 15G.
+
For Public Cloud instances we recommend that the repositories and the {productname} Server database, and respectively the {productname} Proxy squid cache, be located on an external virtual disk.
The details for this setup are provided in <<using-separate-storage-volume>>.
+
Storing the database and the repositories on an external virtual disk ensures that the data is not lost should the instance need to be terminated for some reason.


Please ensure that the selected instance type matches the requirements listed above.
Although we recommend that the database and the repositories are stored on a separate device it is still recommended to set the root volume size of the instance to 20G.

[[setup]]
== Setup


Run an instance of the {productname} Server or {productname} Proxy image as published by SUSE.
The images are identifiable by the suse, manager, server or proxy, and byos keywords in each public cloud environment.
The {productname} instance must run in a network access restricted environment such as the private subnet of a VPC or with an appropriate firewall setting such that it can only be accessed by machines in the IP ranges you use.
A generally accessible {productname} instance violates the terms of the {productname} EULA.
Access to the web interface of SUSE Manager requires https.

{productname} requires a stable and reliable hostname, changing the hostname can create errors.
In this procedure, all commands need to be executed as the root user.

.Procedure: Setting the Hostname
. Disable hostname setup by editing the DHCP configuration file at [path]``/etc/sysconfig/network/dhcp``, and adding this line:
+

----
DHCLIENT_SET_HOSTNAME="no"
----

. Set the hostname locally using the [command]``hostnamectl`` command.
Ensure you use the system name, not the fully qualified domain name (FQDN).
The FQDN is set by the cloud framework; for example if  *cloud_instance.cloud.net* is the FQDN, *cloud_instance* is the system name and *cloud.net* is the domain name:
+

----
hostnamectl set-hostname system_name
----
+

. Create a DNS entry in your network environment for domain name resolution, or force correct resolution by editing the [path]``/etc/hosts`` file:
+

----
$ echo "${local_address} suma.cloud.net suma" >> /etc/hosts
----
+

You can locate the local address by checking your public cloud web console, or from the command line using these commands:
+

*** Amazon EC2 instance:
+

----
$ ec2metadata --local-ipv4
----
*** Google Compute Engine:
+

----
$ gcemetadata --query instance --network-interfaces --ip
----
+

*** Microsoft Azure:
+

----
$ azuremetadata --internal-ip
----


Forcing DNS resolution by modifying the [path]``/etc/hosts`` file will work correctly in most environments.
However, you will have to perform the same modification for any client that is to be managed with this {productname} instance.
In most cases, it will be easier to manage the DNS resolution by creating a DNS entry in your network environment.


One other method for managing hostname resolution is by editing the [path]``/etc/resolv.conf`` file.
Depending on the order of your setup, for example if you started the {productname} instance prior to setting up DNS services the file may not contain the appropriate *search* directive.
Double check that the proper search directive exists in [path]``/etc/resolv.conf``.
In our example the directive would be **search cloud.net**.
If the directive is missing add it to the file.


For an update of the DNS records for the instance within the DNS service of your network environment, refer to the cloud service provider documentation for detailed instructions:
*** http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-dns.html[ DNS setup on Amazon EC2]
*** https://cloud.google.com/compute/docs/networking[ DNS setup on Google Compute Engine]
*** https://azure.microsoft.com/en-us/documentation/articles/dns-operations-recordsets[ DNS setup on Microsoft Azure]

If you run a {productname} Server instance, you can run YaST after the instance is launched to ensure the external storage is attached and prepared according to <<using-separate-storage-volume>>, and the DNS resolution is set up as described:

----
$ /sbin/yast2 susemanagersetup
----


Note that the setup of {productname} from this point forward does not differ from the documentation in the https://www.suse.com/documentation/suse_manager[SUSE Manager Guide].

The {productname} setup procedure in YaST is designed as a one pass process with no rollback or cleanup capability.
Therefore, if the setup procedure is interrupted or ends with an error, it is not recommended that you repeat the setup process or attempts to manually fix the configuration.
These methods are likely to result in a faulty {productname} installation.
If you experience errors during setup, start a new instance, and begin the setup procedure again on a clean system.

If you receive a message that there is not enough space available for setup, ensure that your root volume is at least 20GB and double check that the instructions in <<using-separate-storage-volume>> have been completed correctly.

{productname} Server for the Public Cloud comes with a bootstrap data module pre-installed.
The bootstrap module contains optimized package lists for bootstrapping instances started from {sle} images published by {suse}.
If you intend to register such an instance, when you creatr the bootstrap repository run the [command]``mgr-create-bootstrap-repo`` script using this command, to create a bootstrap repository suitable for {sle} 12 SP1 instances.

----
$ mgr-create-bootstrap-repo --datamodule=mgr_pubcloud_bootstrap_data -c SLE-12-SP1-x86_64
----


See https://www.suse.com/documentation/suse-manager-3/book.suma.getting-started/data/create_tools_repository.html[Creating the SUSE Manager Tools Repository] for more information on bootstrapping.

Prior to registering instances started from on demand images remove the following packages from the instance to be registered:
... cloud-regionsrv-client
... *For Amazon EC2*
+
regionServiceClientConfigEC2
+
regionServiceCertsEC2
... *For Google Compute Engine*
+
cloud-regionsrv-client-plugin-gce
+
regionServiceClientConfigGCE
+
regionServiceCertsGCE
... *For Microsoft Azure*
+
regionServiceClientConfigAzure
+
regionServiceCertsAzure

+
If these packages are not removed it is possible to create interference between the repositories provided by {productname} and the repositories provided by the SUSE operated update infrastructure.
+
Additionally remove the line from the [path]``/etc/hosts``
file that contains the *susecloud.net* reference.
** If you run a {productname} Proxy instance
+
Launch the instance, optionally with external storage configured.
If you use external storage (recommended), prepare it according to <<using-separate-storage-volume>>.
It is recommended but not required to prepare the storage before configuring {productname} proxy, as the suma-storage script will migrate any existing cached data to the external storage.
After preparing the instance, register the system with the parent SUSE Manager, which could be a {productname} Server or another {productname} Proxy.
See the https://www.suse.com/documentation/suse-manager-3/singlehtml/suse_manager21/book_susemanager_proxyquick/book_susemanager_proxyquick.html[ SUSE Manager Proxy Setup guide] for details.
Once registered, run
+

----
$ /usr/sbin/configure-proxy.sh
----
+
to configure your {productname} Proxy instance.
. After the completion of the configuration step, {productname} should be functional and running. For {productname} Server, the setup process created an administrator user with following user name:
+
* User name: `admin`
+

.Account credentials for admin user
[cols="1,1,1", options="header"]
|===
|
          Amazon EC2

|
          Google Compute Engine

|
          Microsoft Azure


|

[replaceable]``Instance-ID``
|

[replaceable]``Instance-ID``
|

[replaceable]``Instance-Name``**-suma**
|===
+
The current value for the [replaceable]``Instance-ID`` or [replaceable]``Instance-Name`` in case of the Azure Cloud, can be obtained from the public cloud Web console or from within a terminal session as follows:
** Obtain instance id from within Amazon EC2 instance
+

----
$ ec2metadata --instance-id
----
** Obtain instance id from within Google Compute Engine instance
+

----
$ gcemetadata --query instance --id
----
** Obtain instance name from within Microsoft Azure instance
+

----
$ azuremetadata --instance-name
----

+
After logging in through the {productname} Server {webui}, *change* the default password.
+
{productname} Proxy does not have administration access to the {webui}.
It can be managed through its parent {productname} Server.


[[using-separate-storage-volume]]
=== Using Separate Storage Volume


We recommend that the repositories and the database for {productname} be stored on a virtual storage device.
This best practice will avoid data loss in cases where the {productname} instance may need to be terminated.
These steps *must* be performed *prior* to running the YaST {productname}  setup procedure.


. Provision a disk device in the public cloud environment, refer to the cloud service provider documentation for detailed instructions. The size of the disk is dependent on the number of distributions and channels you intend to manage with {productname}.
For sizing information refer to https://www.suse.com/support/kb/doc.php?id=7015050[SUSE Manager sizing examples]. A rule of thumb is 25 GB per distribution per channel.
. Once attached the device appears as Unix device node in your instance. For the following command to work this device node name is required. In many cases the attached storage appears as **/dev/sdb**. In order to check which disk devices exists on your system, call the following command:
+

----
$ hwinfo --disk | grep -E "Device File:"
----
. With the device name at hand the process of re-linking the directories in the filesystem {productname} uses to store data is handled by the suma-storage script. In the following example we use [path]``/dev/sdb`` as the device name.
+

----
$ /usr/bin/suma-storage /dev/sdb
----
+
After the call all database and repository files used by SUSE Manager Server are moved to the newly created xfs based storage.
In case your instance is a {productname} Proxy, the script will move the Squid cache, which caches the software packages, to the newly created storage.
The xfs partition is mounted below the path [path]``/manager_storage``.
.
. Create an entry in /etc/fstab (optional)
+
Different cloud frameworks treat the attachment of external storage devices differently at instance boot time.
Please refer to the cloud environment documentation for guidance about the fstab entry.
+
If your cloud framework recommends to add an fstab entry, add the following line to the */etc/fstab* file.
+

----
/dev/sdb1 /manager_storage xfs defaults 1 1
----


[[registration-of-cloned-systems]]
== Registration of Cloned Systems

{productname} cannot distinguish between different instances that use the same system ID.
If you register a second instance with the same system ID as a previous instance, {productname} will overwrite the original system data with the new system data.
This can occur when you launch multiple instances from the same image, or when an image is created from a running instance.
However, it is possible to clone systems and register them successfully by deleting the cloned system's ID, and generating a new ID.


.Procedure: Registering Cloned Systems
. Clone the system using your preferred hypervisor's cloning mechanism.
. On the cloned system, change the hostname and IP addresses, and check the [path]``/etc/hosts`` file to ensure you have the right host entries.
. Stop the [command]``rhnsd`` daemon with [command]``/etc/init.d/rhnsd stop`` or [command]``rcosad stop``.
. For {slsa} 11 or {rhel} 5 or 6 clients, run these commands:
+
----
# rm /var/lib/dbus/machine-id
# dbus-uuidgen --ensure
----
+
. For {slsa} 12 or {rhel} 7 clients, run these commands:
+
----
# rm /etc/machine-id
# rm /var/lib/dbus/machine-id
# dbus-uuidgen --ensure
# systemd-machine-id-setup
----
+
. If you are using Salt, then you will also need to run these commands:
+
----
# service salt-minion stop
# rm -rf /var/cache/salt
----
+
. If you are using a traditional registered system, clean up the working files with this command:
+
----
# rm -f /etc/sysconfig/rhn/{osad-auth.conf,systemid}
----

The bootstrap should now run with a new system ID, rather than a duplicate.


If you are onboarding Salt minion clones, then you will also need to check if they have the same Salt minion ID.
You will need to delete the minion ID on each cloned minion, using the [command]``rm`` command.
Each operating system type stores this file in a slightly different location, check the table for the appropriate command.


.Minion ID File Location
Each operating system stores the minion ID file in a slightly different location, check the table for the appropriate command.

[cols="1,1", options="header"]
|===
| Operating System | Commands
| {slsa} 12        | [command]``rm /etc/salt/minion_id``

                     [command]``rm  -f /etc/zypp/credentials.d/{SCCcredentials,NCCcredentials}``
| {slsa} 11        | [command]``rm /etc/salt/minion_id``

                     [command]``suse_register -E``
| {slsa} 10        | [command]``rm -rf /etc/{zmd,zypp}``

                     [command]``rm -rf /var/lib/zypp/``
                     Do not delete [path]``/var/lib/zypp/db/products/``

                     [command]``rm -rf /var/lib/zmd/``
| {rhel} 5, 6, 7   | [command]`` rm  -f /etc/NCCcredentials``
|===


Once you have deleted the minion ID file, re-run the bootstrap script, and restart the minion to see the cloned system in {productname} with the new ID.
